# å™ªéŸ³è‡ªé€‚åº”UNSBå®æ–½æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥: ä¼°è®¡æ•°æ®å™ªéŸ³æ°´å¹³

åœ¨è®­ç»ƒä¹‹å‰ï¼Œå…ˆåˆ†æä½ çš„æ•°æ®é›†çš„å™ªéŸ³æ°´å¹³:

```bash
python -c "
from noise_estimation import analyze_dataset_noise
import json

# åˆ†æåŸŸAå’ŒåŸŸBçš„å™ªéŸ³
stats_A = analyze_dataset_noise('./datasets/YOUR_DATASET', domain='A', num_samples=50)
stats_B = analyze_dataset_noise('./datasets/YOUR_DATASET', domain='B', num_samples=50)

print('Domain A (PD):')
print(json.dumps(stats_A, indent=2))
print('\nDomain B (PDFs):')
print(json.dumps(stats_B, indent=2))

# ä¿å­˜ç»“æœ
with open('noise_analysis.json', 'w') as f:
    json.dump({'A': stats_A, 'B': stats_B}, f, indent=2)

# æ¨èçš„data_noise_levelå‚æ•°
recommended_noise = max(stats_A['median_noise'], stats_B['median_noise'])
print(f'\næ¨èä½¿ç”¨ --data_noise_level {recommended_noise:.4f}')
"
```

**é¢„æœŸè¾“å‡ºç¤ºä¾‹**:
```json
{
  "A": {
    "mean_noise": 0.0234,
    "median_noise": 0.0198,
    "std_noise": 0.0087,
    "mean_snr": 45.2,
    "median_snr": 48.1
  },
  "B": {
    "mean_noise": 0.0312,
    "median_noise": 0.0287,
    "std_noise": 0.0102,
    "mean_snr": 38.7,
    "median_snr": 41.2
  }
}

æ¨èä½¿ç”¨ --data_noise_level 0.0287
```

---

### ç¬¬äºŒæ­¥: ä¿®æ”¹ä»£ç 

éœ€è¦ä¿®æ”¹ä»¥ä¸‹æ–‡ä»¶:
1. `options/base_options.py` - æ·»åŠ æ–°å‚æ•°
2. `models/sb_model.py` - å®ç°è‡ªé€‚åº”æŸå¤±

#### ä¿®æ”¹1: options/base_options.py

åœ¨`BaseOptions.initialize()`ä¸­æ·»åŠ å™ªéŸ³ç›¸å…³å‚æ•°:

```python
# åœ¨æ–‡ä»¶æœ«å°¾çš„parser.add_argumentä¹‹åæ·»åŠ :

# === å™ªéŸ³å¤„ç†å‚æ•° (Nila-inspired) ===
parser.add_argument('--data_noise_level', type=float, default=0.0,
                   help='Estimated noise level in the data (Ïƒ_y). '
                        'Set to 0 to disable noise-adaptive loss. '
                        'Use noise_estimation.py to estimate this value.')

parser.add_argument('--noise_adaptive_schedule', type=str, default='linear',
                   choices=['linear', 'exponential', 'step', 'none'],
                   help='Schedule for noise-adaptive weight decay.')

parser.add_argument('--noise_adaptive_start_epoch', type=int, default=0,
                   help='Start applying noise-adaptive loss after this epoch. '
                        'Useful for curriculum learning.')

parser.add_argument('--visualize_noise_schedule', action='store_true',
                   help='Visualize the noise-adaptive schedule at start of training.')
```

#### ä¿®æ”¹2: models/sb_model.py

åœ¨`SBModel`ç±»ä¸­ä¿®æ”¹`compute_G_loss()`æ–¹æ³•:

```python
def compute_G_loss(self):
    """Calculate GAN and NCE loss for the generator"""
    bs = self.real_A.size(0)
    tau = self.opt.tau

    # === æ–°å¢: è®¡ç®—å™ªéŸ³è‡ªé€‚åº”æƒé‡ ===
    if self.opt.data_noise_level > 0:
        t = self.time_idx[0].item()
        T = self.opt.num_timesteps

        # è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„äººå·¥å™ªéŸ³æ°´å¹³
        t_normalized = t / T
        artificial_noise = np.sqrt(tau * t_normalized * (1 - t_normalized))

        # å™ªéŸ³æ¯”ç‡
        noise_ratio = artificial_noise / (self.opt.data_noise_level + 1e-8)

        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        if noise_ratio >= 1.0:
            noise_adaptive_weight = 1.0
        else:
            if self.opt.noise_adaptive_schedule == 'linear':
                noise_adaptive_weight = noise_ratio
            elif self.opt.noise_adaptive_schedule == 'exponential':
                noise_adaptive_weight = np.exp(-3.0 * (1 - noise_ratio))
            elif self.opt.noise_adaptive_schedule == 'step':
                noise_adaptive_weight = 1.0 if noise_ratio > 0.5 else 0.0
            else:
                noise_adaptive_weight = 1.0

        noise_adaptive_weight = float(np.clip(noise_adaptive_weight, 0.0, 1.0))
    else:
        noise_adaptive_weight = 1.0

    # å­˜å‚¨ç”¨äºç›‘æ§
    self.noise_adaptive_weight = noise_adaptive_weight

    # === GANæŸå¤± ===
    fake = self.fake_B
    std = torch.rand(size=[1]).item() * self.opt.std

    if self.opt.lambda_GAN > 0.0:
        pred_fake = self.netD(fake, self.time_idx)
        self.loss_G_GAN = self.criterionGAN(pred_fake, True).mean() * self.opt.lambda_GAN
    else:
        self.loss_G_GAN = 0.0

    # === SBæŸå¤± (ä¿®æ”¹éƒ¨åˆ†) ===
    self.loss_SB = 0
    if self.opt.lambda_SB > 0.0:
        XtXt_1 = torch.cat([self.real_A_noisy, self.fake_B], dim=1)
        XtXt_2 = torch.cat([self.real_A_noisy2, self.fake_B2], dim=1)

        bs = self.opt.batch_size

        # èƒ½é‡é¡¹ (ä¸å—å™ªéŸ³å½±å“ï¼Œä¿æŒä¸å˜)
        ET_XY = self.netE(XtXt_1, self.time_idx, XtXt_1).mean() \
              - torch.logsumexp(self.netE(XtXt_1, self.time_idx, XtXt_2).reshape(-1), dim=0)

        energy_term = -(self.opt.num_timesteps - self.time_idx[0]) / self.opt.num_timesteps * self.opt.tau * ET_XY

        # ğŸ”¥ é‡å»ºé¡¹ (åº”ç”¨å™ªéŸ³è‡ªé€‚åº”æƒé‡)
        reconstruction_loss = torch.mean((self.real_A_noisy - self.fake_B)**2)
        reconstruction_term = noise_adaptive_weight * self.opt.tau * reconstruction_loss

        self.loss_SB = energy_term + reconstruction_term

        # å­˜å‚¨åˆ†è§£ç”¨äºç›‘æ§
        self.loss_SB_energy = energy_term
        self.loss_SB_recon = reconstruction_term

    # === NCEæŸå¤± ===
    if self.opt.lambda_NCE > 0.0:
        self.loss_NCE = self.calculate_NCE_loss(self.real_A, fake)
    else:
        self.loss_NCE, self.loss_NCE_bd = 0.0, 0.0

    if self.opt.nce_idt and self.opt.lambda_NCE > 0.0:
        self.loss_NCE_Y = self.calculate_NCE_loss(self.real_B, self.idt_B)
        loss_NCE_both = (self.loss_NCE + self.loss_NCE_Y) * 0.5
    else:
        loss_NCE_both = self.loss_NCE

    # === æ€»æŸå¤± ===
    self.loss_G = self.loss_G_GAN + self.opt.lambda_SB * self.loss_SB + self.opt.lambda_NCE * loss_NCE_both
    return self.loss_G
```

#### ä¿®æ”¹3: æ·»åŠ ç›‘æ§ (å¯é€‰ä½†æ¨è)

åœ¨`sb_model.py`çš„`__init__`ä¸­æ·»åŠ :

```python
def __init__(self, opt):
    BaseModel.__init__(self, opt)

    # åŸæœ‰ä»£ç ...
    self.loss_names = ['G_GAN', 'D_real', 'D_fake', 'G', 'NCE', 'SB']

    # ğŸ”¥ æ–°å¢: æ·»åŠ å™ªéŸ³è‡ªé€‚åº”ç›¸å…³çš„ç›‘æ§é¡¹
    if opt.data_noise_level > 0:
        self.loss_names += ['SB_energy', 'SB_recon']
        # æ³¨æ„: noise_adaptive_weight ä¸æ˜¯æŸå¤±ï¼Œä½†ä¼šåœ¨è®­ç»ƒæ—¶æ‰“å°

    # ... å…¶ä½™ä»£ç 
```

åœ¨wandbæ—¥å¿—ä¸­æ·»åŠ å™ªéŸ³æƒé‡ç›‘æ§ (ä¿®æ”¹ `util/wandb_logger.py`):

```python
# åœ¨log_current_lossesä¸­æ·»åŠ :
if hasattr(model, 'noise_adaptive_weight'):
    wandb_log['train/noise_adaptive_weight'] = model.noise_adaptive_weight
```

---

### ç¬¬ä¸‰æ­¥: è®­ç»ƒ

#### åŸºç¡€è®­ç»ƒå‘½ä»¤

```bash
python train.py \
  --dataroot ./datasets/PD_PDFS \
  --name experiment_noise_adaptive \
  --model sb \
  --dataset_mode mri_unaligned \
  --mri_representation real_imag \
  --mri_normalize_per_case \
  --mri_normalize_method percentile_95 \
  --data_noise_level 0.03 \
  --noise_adaptive_schedule linear \
  --wandb_project mri-contrast-transfer-noise \
  --batch_size 4 \
  --n_epochs 200 \
  --n_epochs_decay 200
```

#### å¯¹æ¯”å®éªŒ

**å®éªŒ1: Baseline (æ— å™ªéŸ³å¤„ç†)**
```bash
python train.py \
  --name baseline_no_noise_handling \
  --data_noise_level 0.0 \
  # ... å…¶ä»–å‚æ•°ç›¸åŒ
```

**å®éªŒ2: çº¿æ€§è¡°å‡**
```bash
python train.py \
  --name noise_adaptive_linear \
  --data_noise_level 0.03 \
  --noise_adaptive_schedule linear \
  # ... å…¶ä»–å‚æ•°ç›¸åŒ
```

**å®éªŒ3: æŒ‡æ•°è¡°å‡**
```bash
python train.py \
  --name noise_adaptive_exponential \
  --data_noise_level 0.03 \
  --noise_adaptive_schedule exponential \
  # ... å…¶ä»–å‚æ•°ç›¸åŒ
```

**å®éªŒ4: ä¸åŒå™ªéŸ³æ°´å¹³**
```bash
# ä½ä¼°å™ªéŸ³
python train.py \
  --name noise_level_0.01 \
  --data_noise_level 0.01 \
  # ...

# å‡†ç¡®ä¼°è®¡
python train.py \
  --name noise_level_0.03 \
  --data_noise_level 0.03 \
  # ...

# é«˜ä¼°å™ªéŸ³
python train.py \
  --name noise_level_0.05 \
  --data_noise_level 0.05 \
  # ...
```

---

### ç¬¬å››æ­¥: ç›‘æ§è®­ç»ƒ

åœ¨Wandbä¸­å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡:

1. **noise_adaptive_weight**: åº”è¯¥åœ¨æ—©æœŸepochsæ¥è¿‘1.0ï¼Œæ™šæœŸæ—¶é—´æ­¥é€æ¸é™ä½
2. **loss_SB_energy vs loss_SB_recon**:
   - èƒ½é‡é¡¹åº”è¯¥ä¿æŒç¨³å®š
   - é‡å»ºé¡¹ä¼šå—è‡ªé€‚åº”æƒé‡å½±å“
3. **ç”Ÿæˆè´¨é‡**: è§‚å¯Ÿç”Ÿæˆå›¾åƒçš„å™ªéŸ³æ°´å¹³æ˜¯å¦é™ä½

**é¢„æœŸç°è±¡**:
- æ—©æœŸè®­ç»ƒ: weight â‰ˆ 1.0 â†’ æ­£å¸¸SBè®­ç»ƒ
- ä¸­åæœŸè®­ç»ƒ: weightåœ¨0.5-1.0ä¹‹é—´æ³¢åŠ¨ â†’ è‡ªé€‚åº”è°ƒæ•´
- æ™šæœŸå°tæ­¥: weight â‰ˆ 0.2-0.5 â†’ å‡å°‘å™ªéŸ³æ‹Ÿåˆ

---

## è¿›é˜¶ä¼˜åŒ–

### ä¼˜åŒ–1: è¯¾ç¨‹å­¦ä¹ 

å…ˆç”¨å…¨å¼ºåº¦è®­ç»ƒï¼Œå†é€æ¸å¯ç”¨å™ªéŸ³è‡ªé€‚åº”:

```bash
python train.py \
  --data_noise_level 0.03 \
  --noise_adaptive_start_epoch 50 \
  # ... å…¶ä»–å‚æ•°
```

**åŸç†**: è®©æ¨¡å‹å…ˆå­¦ä¹ åŸºæœ¬çš„å¯¹æ¯”åº¦æ˜ å°„ï¼Œå†fine-tuneå»å™ª

### ä¼˜åŒ–2: åŠ¨æ€å™ªéŸ³ä¼°è®¡

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å™ªéŸ³æ°´å¹³:

```python
# åœ¨sb_model.pyçš„optimize_parametersä¸­æ·»åŠ :
def optimize_parameters(self):
    # ... åŸæœ‰ä»£ç 

    # æ¯Nä¸ªiterationæ›´æ–°å™ªéŸ³ä¼°è®¡
    if self.opt.dynamic_noise_estimation and self.total_iters % 1000 == 0:
        # ä½¿ç”¨å½“å‰ç”Ÿæˆå™¨ä¼°è®¡æ•°æ®å™ªéŸ³
        with torch.no_grad():
            # åœ¨t=0æ—¶åˆ»ï¼Œfake_Båº”è¯¥æ¥è¿‘real_A (å¦‚æœæ— å™ªéŸ³)
            residual = self.real_A - self.netG(self.real_A,
                                               torch.zeros_like(self.time_idx),
                                               torch.randn_like(z))
            estimated_noise = torch.std(residual).item()

            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            alpha = 0.1
            self.opt.data_noise_level = (1 - alpha) * self.opt.data_noise_level \
                                       + alpha * estimated_noise

            print(f"Updated data_noise_level to {self.opt.data_noise_level:.4f}")
```

### ä¼˜åŒ–3: ç©ºé—´è‡ªé€‚åº”

ä¸åŒåŒºåŸŸçš„å™ªéŸ³æ°´å¹³å¯èƒ½ä¸åŒ (èƒŒæ™¯ vs ä¿¡å·åŒºåŸŸ):

```python
def compute_spatial_adaptive_weight(self, real_A_noisy, fake_B):
    """
    è®¡ç®—ç©ºé—´å˜åŒ–çš„è‡ªé€‚åº”æƒé‡
    èƒŒæ™¯åŒºåŸŸ(ä½ä¿¡å·): æ›´ä½çš„æƒé‡
    ä¿¡å·åŒºåŸŸ(é«˜ä¿¡å·): æ›´é«˜çš„æƒé‡
    """
    # è®¡ç®—magnitude
    mag_A = torch.sqrt(real_A_noisy[:, 0]**2 + real_A_noisy[:, 1]**2)

    # å½’ä¸€åŒ–åˆ°[0, 1]
    mag_A_norm = (mag_A - mag_A.min()) / (mag_A.max() - mag_A.min() + 1e-8)

    # ç©ºé—´æƒé‡: ä¿¡å·å¼ºåº¦è¶Šé«˜ï¼Œæƒé‡è¶Šé«˜
    spatial_weight = mag_A_norm.unsqueeze(1)  # [B, 1, H, W]

    # ç»“åˆæ—¶é—´è‡ªé€‚åº”æƒé‡
    combined_weight = self.noise_adaptive_weight * spatial_weight

    # åŠ æƒæŸå¤±
    reconstruction_loss = torch.mean(
        combined_weight * (real_A_noisy - fake_B)**2
    )

    return reconstruction_loss
```

---

## è¯„ä¼°

### å®šé‡è¯„ä¼°è„šæœ¬

åˆ›å»º `evaluate_noise.py`:

```python
import torch
import numpy as np
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
from noise_estimation import estimate_noise_mad

def evaluate_model(opt, model, test_dataset):
    """è¯„ä¼°æ¨¡å‹åœ¨å™ªéŸ³æŒ‡æ ‡ä¸Šçš„è¡¨ç°"""

    noise_levels_input = []
    noise_levels_output = []
    psnr_scores = []
    ssim_scores = []

    model.eval()
    with torch.no_grad():
        for i, data in enumerate(test_dataset):
            model.set_input(data)
            model.forward()

            # è®¡ç®—è¾“å…¥å’Œè¾“å‡ºçš„å™ªéŸ³æ°´å¹³
            real_A_mag = torch.sqrt(model.real_A[0, 0]**2 + model.real_A[0, 1]**2).cpu().numpy()
            fake_B_mag = torch.sqrt(model.fake_B[0, 0]**2 + model.fake_B[0, 1]**2).cpu().numpy()

            noise_input = estimate_noise_mad(real_A_mag)
            noise_output = estimate_noise_mad(fake_B_mag)

            noise_levels_input.append(noise_input)
            noise_levels_output.append(noise_output)

            # å¦‚æœæœ‰å‚è€ƒå›¾åƒ
            if hasattr(model, 'real_B'):
                real_B_mag = torch.sqrt(model.real_B[0, 0]**2 + model.real_B[0, 1]**2).cpu().numpy()

                # å½’ä¸€åŒ–åˆ°[0, 1]ç”¨äºPSNR/SSIMè®¡ç®—
                fake_B_norm = (fake_B_mag - fake_B_mag.min()) / (fake_B_mag.max() - fake_B_mag.min())
                real_B_norm = (real_B_mag - real_B_mag.min()) / (real_B_mag.max() - real_B_mag.min())

                psnr_score = psnr(real_B_norm, fake_B_norm, data_range=1.0)
                ssim_score = ssim(real_B_norm, fake_B_norm, data_range=1.0)

                psnr_scores.append(psnr_score)
                ssim_scores.append(ssim_score)

    results = {
        'mean_noise_input': np.mean(noise_levels_input),
        'mean_noise_output': np.mean(noise_levels_output),
        'noise_reduction_ratio': np.mean(noise_levels_output) / np.mean(noise_levels_input),
        'mean_psnr': np.mean(psnr_scores) if psnr_scores else None,
        'mean_ssim': np.mean(ssim_scores) if ssim_scores else None
    }

    return results
```

è¿è¡Œè¯„ä¼°:
```bash
python test.py \
  --name noise_adaptive_linear \
  --epoch latest \
  # ... å…¶ä»–å‚æ•°

python -c "
from evaluate_noise import evaluate_model
# åŠ è½½æ¨¡å‹å’Œæ•°æ®é›†
results = evaluate_model(opt, model, test_dataset)
print(results)
"
```

**æœŸæœ›ç»“æœ**:
```python
{
    'mean_noise_input': 0.0287,
    'mean_noise_output': 0.0134,  # å™ªéŸ³å‡å°‘äº†çº¦53%
    'noise_reduction_ratio': 0.47,
    'mean_psnr': 32.4,
    'mean_ssim': 0.89
}
```

å¯¹æ¯”baseline:
```python
# Baseline (æ— å™ªéŸ³å¤„ç†)
{
    'mean_noise_output': 0.0298,  # å™ªéŸ³å‡ ä¹æ²¡å˜
    'noise_reduction_ratio': 1.04,
    'mean_psnr': 28.7,
    'mean_ssim': 0.82
}
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: è®­ç»ƒä¸ç¨³å®š

**ç—‡çŠ¶**: æŸå¤±å‰§çƒˆæ³¢åŠ¨ï¼Œç”Ÿæˆè´¨é‡å·®

**å¯èƒ½åŸå› **:
- `data_noise_level` è®¾ç½®è¿‡é«˜
- è‡ªé€‚åº”æƒé‡å˜åŒ–è¿‡å¿«

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é™ä½å™ªéŸ³æ°´å¹³ä¼°è®¡
--data_noise_level 0.01  # è€Œé0.03

# ä½¿ç”¨æ›´å¹³æ»‘çš„è¡°å‡
--noise_adaptive_schedule exponential

# å»¶è¿Ÿå¯ç”¨è‡ªé€‚åº”
--noise_adaptive_start_epoch 100
```

### é—®é¢˜2: å™ªéŸ³å‡å°‘ä¸æ˜æ˜¾

**ç—‡çŠ¶**: è¾“å‡ºå›¾åƒå™ªéŸ³æ°´å¹³å’Œè¾“å…¥ç›¸è¿‘

**å¯èƒ½åŸå› **:
- `data_noise_level` è®¾ç½®è¿‡ä½
- è‡ªé€‚åº”æƒé‡å‡ ä¹æ€»æ˜¯1.0

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æé«˜å™ªéŸ³æ°´å¹³ä¼°è®¡
--data_noise_level 0.05

# ä½¿ç”¨æ›´æ¿€è¿›çš„è¡°å‡
--noise_adaptive_schedule linear
```

### é—®é¢˜3: è¿‡åº¦å¹³æ»‘

**ç—‡çŠ¶**: è¾“å‡ºå›¾åƒç»†èŠ‚ä¸¢å¤±

**å¯èƒ½åŸå› **:
- `data_noise_level` è®¾ç½®è¿‡é«˜
- è‡ªé€‚åº”æƒé‡è¿‡æ—©é™ä¸º0

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é™ä½å™ªéŸ³æ°´å¹³
--data_noise_level 0.02

# æ£€æŸ¥å¯è§†åŒ–
--visualize_noise_schedule
```

---

## æ€»ç»“

**æ ¸å¿ƒæ€æƒ³**: å€Ÿé‰´Nilaçš„å™ªéŸ³æ°´å¹³è‡ªé€‚åº”ç­–ç•¥ï¼Œåœ¨SBæ¡†æ¶ä¸­åŠ¨æ€è°ƒæ•´é‡å»ºæŸå¤±çš„æƒé‡

**å…³é”®å‚æ•°**:
- `data_noise_level`: æœ€é‡è¦ï¼éœ€è¦ä»æ•°æ®ä¸­å‡†ç¡®ä¼°è®¡
- `noise_adaptive_schedule`: linearé€šå¸¸æ•ˆæœæœ€å¥½
- `tau`: SBçš„å™ªéŸ³å‚æ•°ï¼Œä¸data_noise_levelé…åˆä½¿ç”¨

**é¢„æœŸæ•ˆæœ**:
- å™ªéŸ³å‡å°‘30-50%
- PSNRæå‡2-4 dB
- SSIMæå‡0.05-0.10
- è§†è§‰è´¨é‡æ˜¾è‘—æ”¹å–„

**ä¸‹ä¸€æ­¥**:
å¦‚æœåŸºç¡€æ–¹æ¡ˆæ•ˆæœä¸å¤Ÿç†æƒ³,å¯ä»¥å°è¯•:
1. æ–¹æ¡ˆ3: å™ªéŸ³æ¡ä»¶åŒ–ç”Ÿæˆå™¨
2. ä¼˜åŒ–1: è¯¾ç¨‹å­¦ä¹ 
3. ä¼˜åŒ–3: ç©ºé—´è‡ªé€‚åº”æƒé‡
