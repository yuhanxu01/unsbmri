# å½“èµ·ç‚¹å’Œç»ˆç‚¹éƒ½æœ‰å™ªéŸ³æ—¶çš„å»å™ªç­–ç•¥

## é—®é¢˜åœºæ™¯

ä½ çš„æƒ…å†µï¼š
```
æºåŸŸ (PD):   å«å™ªéŸ³, Ïƒ_A â‰ˆ 0.03
ç›®æ ‡åŸŸ (PDFs): å«å™ªéŸ³, Ïƒ_B â‰ˆ 0.03

ä»»åŠ¡: PD â†’ PDFs å¯¹æ¯”åº¦è¿ç§»
æœŸæœ›: è¾“å‡ºæ¯”è¾“å…¥æ›´å¹²å‡€
```

è¿™ä¸Nilaçš„åŸå§‹åœºæ™¯ä¸åŒï¼š
- Nila: å«å™ªæµ‹é‡ â†’ å¹²å‡€å›¾åƒ (æœ‰æ˜ç¡®çš„å»å™ªç›®æ ‡)
- ä½ : å«å™ªæº â†’ å«å™ªç›®æ ‡ (æ²¡æœ‰"å¹²å‡€å‚è€ƒ")

---

## åŸºç¡€æ–¹æ¡ˆçš„æ•ˆæœåˆ†æ

### æ–¹æ¡ˆ2 (Nilaå¯å‘çš„è‡ªé€‚åº”SBæŸå¤±)

**èƒ½åšåˆ°**:
- âœ… å‡å°‘å™ªéŸ³ä¼ é€’ (30-50%å™ªéŸ³å‡å°‘)
- âœ… éšå¼å»å™ª (é€šè¿‡å…ˆéªŒçŸ¥è¯†)
- âœ… é¿å…å­¦ä¹ å™ªéŸ³æ¨¡å¼

**å±€é™æ€§**:
- âŒ æ— æ³•å®Œå…¨æ¶ˆé™¤å™ªéŸ³
- âŒ å—é™äºç›®æ ‡åŸŸæ•°æ®è´¨é‡
- âŒ æ— æ˜¾å¼å»å™ªç›‘ç£

**æ•°å­¦è§£é‡Š**:

```python
# æ ‡å‡†SBè®­ç»ƒ
min_G E[ ||X_t - G(X_t)||Â² ]
# X_tåŒ…å«è¾“å…¥å™ªéŸ³ â†’ Gå­¦ä¹ å¤åˆ¶å™ªéŸ³

# è‡ªé€‚åº”SB
min_G E[ Î»_t * ||X_t - G(X_t)||Â² ]
# Î»_t < 1 å‡å¼±å™ªéŸ³æ‹Ÿåˆ â†’ éƒ¨åˆ†å»å™ª

# ä½†GANæŸå¤±ä»ç„¶æ˜¯:
min_G E[ D(G(X)) - D(real_B) ]
# real_Bæœ‰å™ªéŸ³ â†’ Gè¢«é¼“åŠ±ç”Ÿæˆ"é€‚åº¦å™ªéŸ³"
```

**é¢„æœŸæ•ˆæœ**:
```
è¾“å…¥PD:  Ïƒ = 0.030
è¾“å‡ºPDFs: Ïƒ = 0.015-0.020 (æ”¹å–„40-50%)

å¯¹æ¯”baseline:
è¾“å…¥PD:  Ïƒ = 0.030
è¾“å‡ºPDFs: Ïƒ = 0.028-0.032 (å‡ ä¹ä¸å˜)
```

---

## å¢å¼ºæ–¹æ¡ˆï¼šç»„åˆå¤šç§ç­–ç•¥

### ğŸ¥‡ æ–¹æ¡ˆA: è‡ªé€‚åº”SB + åˆ¤åˆ«å™¨å»å™ªå¼•å¯¼

**æ ¸å¿ƒæ€æƒ³**: è®©åˆ¤åˆ«å™¨åå¥½æ›´å¹²å‡€çš„å›¾åƒï¼Œè€Œä¸ä»…ä»…æ˜¯"çœŸå®"

#### å®ç°æ–¹æ³•

**A1: æ•°æ®å¢å¼º - åˆæˆå¹²å‡€æ ·æœ¬**

```python
# åœ¨æ•°æ®åŠ è½½æ—¶
class MriUnalignedDataset:
    def __getitem__(self, index):
        A_tensor = self._load_slice(...)  # å«å™ªPD
        B_tensor = self._load_slice(...)  # å«å™ªPDFs

        # ğŸ”¥ æ–°å¢: ç”Ÿæˆ"ä¼ªå¹²å‡€"æ ·æœ¬
        if self.opt.denoise_augmentation and random.random() < 0.5:
            # æ–¹æ³•1: ä¼ ç»Ÿå»å™ª (BM3D, NLM)
            B_tensor_clean = self._traditional_denoise(B_tensor)

            # æ–¹æ³•2: ä½é€šæ»¤æ³¢
            B_tensor_clean = self._lowpass_filter(B_tensor, sigma=1.5)

            # æ–¹æ³•3: Waveletè½¯é˜ˆå€¼
            B_tensor_clean = self._wavelet_denoise(B_tensor)

            # æ··åˆ: 50%åŸå§‹, 50%å»å™ª
            return {
                'A': A_tensor,
                'B': B_tensor_clean,  # åˆ¤åˆ«å™¨å­¦ä¹ åå¥½å¹²å‡€æ ·æœ¬
                'B_original': B_tensor  # ä¿ç•™ç”¨äºå…¶ä»–æŸå¤±
            }

        return {'A': A_tensor, 'B': B_tensor}

def _lowpass_filter(self, tensor, sigma=1.5):
    """ç®€å•çš„é«˜æ–¯ä½é€šæ»¤æ³¢"""
    from scipy.ndimage import gaussian_filter

    if tensor.shape[0] == 2:  # real/imag
        real_filtered = gaussian_filter(tensor[0].numpy(), sigma=sigma)
        imag_filtered = gaussian_filter(tensor[1].numpy(), sigma=sigma)
        return torch.from_numpy(np.stack([real_filtered, imag_filtered]))
    else:  # magnitude
        mag_filtered = gaussian_filter(tensor[0].numpy(), sigma=sigma)
        return torch.from_numpy(mag_filtered[None, ...])

def _wavelet_denoise(self, tensor, wavelet='db4', level=3):
    """Waveletè½¯é˜ˆå€¼å»å™ª"""
    import pywt

    if tensor.shape[0] == 2:
        real_denoised = pywt.threshold(
            pywt.wavedec2(tensor[0].numpy(), wavelet, level=level),
            value=0.1, mode='soft'
        )
        imag_denoised = pywt.threshold(
            pywt.wavedec2(tensor[1].numpy(), wavelet, level=level),
            value=0.1, mode='soft'
        )
        return torch.from_numpy(np.stack([
            pywt.waverec2(real_denoised, wavelet),
            pywt.waverec2(imag_denoised, wavelet)
        ]))
    else:
        coeffs = pywt.wavedec2(tensor[0].numpy(), wavelet, level=level)
        coeffs_thresh = pywt.threshold(coeffs, value=0.1, mode='soft')
        mag_denoised = pywt.waverec2(coeffs_thresh, wavelet)
        return torch.from_numpy(mag_denoised[None, ...])
```

**è®­ç»ƒå‘½ä»¤**:
```bash
python train.py \
  --denoise_augmentation \
  --denoise_method lowpass \
  --denoise_prob 0.5 \
  --data_noise_level 0.03 \
  # ... å…¶ä»–å‚æ•°
```

**æ•ˆæœ**:
- åˆ¤åˆ«å™¨å­¦ä¹ : "å¹²å‡€çš„PDFs > å«å™ªçš„PDFs"
- ç”Ÿæˆå™¨è¢«å¼•å¯¼ç”Ÿæˆæ›´å¹²å‡€çš„å›¾åƒ
- é¢„æœŸå™ªéŸ³å‡å°‘: **50-70%**

---

**A2: å™ªéŸ³æ°´å¹³æ¡ä»¶åˆ¤åˆ«å™¨**

```python
# models/networks.py - ä¿®æ”¹åˆ¤åˆ«å™¨

class NoisyConditionalDiscriminator(nn.Module):
    """
    åˆ¤åˆ«å™¨åŒæ—¶åˆ¤æ–­:
    1. çœŸå® vs ç”Ÿæˆ
    2. å¹²å‡€ vs å«å™ª
    """
    def __init__(self, input_nc, ndf=64):
        super().__init__()

        # ä¸»åˆ¤åˆ«å™¨ç½‘ç»œ
        self.main = nn.Sequential(
            # ... æ ‡å‡†PatchGAN layers
        )

        # ğŸ”¥ æ–°å¢: å™ªéŸ³æ°´å¹³ä¼°è®¡åˆ†æ”¯
        self.noise_estimator = nn.Sequential(
            nn.Conv2d(ndf*8, 128, 1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 1),
            nn.Sigmoid()  # è¾“å‡º[0,1], 0=å¹²å‡€, 1=å¾ˆåµ
        )

    def forward(self, x, t=None):
        features = self.main(x)

        # çœŸå®æ€§åˆ¤æ–­
        validity = self.get_validity(features)

        # å™ªéŸ³æ°´å¹³ä¼°è®¡
        noise_level = self.noise_estimator(features)

        return validity, noise_level

# models/sb_model.py - ä¿®æ”¹åˆ¤åˆ«å™¨æŸå¤±

def compute_D_loss(self):
    # çœŸå®æ•°æ®
    pred_real, noise_real = self.netD(self.real_B, self.time_idx)
    loss_D_real = self.criterionGAN(pred_real, True).mean()

    # ğŸ”¥ æ–°å¢: çœŸå®æ•°æ®çš„å™ªéŸ³æ°´å¹³åº”è¯¥è¾ƒé«˜
    target_noise_real = torch.ones_like(noise_real) * 0.5  # ä¸­ç­‰å™ªéŸ³
    loss_noise_real = F.mse_loss(noise_real, target_noise_real)

    # ç”Ÿæˆæ•°æ®
    pred_fake, noise_fake = self.netD(self.fake_B.detach(), self.time_idx)
    loss_D_fake = self.criterionGAN(pred_fake, False).mean()

    # ğŸ”¥ ç”Ÿæˆæ•°æ®åº”è¯¥æ›´å¹²å‡€
    target_noise_fake = torch.zeros_like(noise_fake)  # æœŸæœ›å¹²å‡€
    loss_noise_fake = F.mse_loss(noise_fake, target_noise_fake)

    self.loss_D = (loss_D_real + loss_D_fake) * 0.5 + \
                  (loss_noise_real + loss_noise_fake) * self.opt.lambda_noise
    return self.loss_D

def compute_G_loss(self):
    # ... åŸæœ‰æŸå¤±

    # ğŸ”¥ æ–°å¢: é¼“åŠ±ç”Ÿæˆå™¨äº§ç”Ÿå¹²å‡€å›¾åƒ
    pred_fake, noise_fake = self.netD(self.fake_B, self.time_idx)

    # GANæŸå¤±: æ¬ºéª—åˆ¤åˆ«å™¨(çœŸå®æ€§)
    self.loss_G_GAN = self.criterionGAN(pred_fake, True).mean()

    # ğŸ”¥ å»å™ªæŸå¤±: ä½¿noise_fakeæ¥è¿‘0 (å¹²å‡€)
    self.loss_G_denoise = torch.mean(noise_fake)

    self.loss_G = self.loss_G_GAN + \
                  self.opt.lambda_SB * self.loss_SB + \
                  self.opt.lambda_NCE * self.loss_NCE + \
                  self.opt.lambda_denoise * self.loss_G_denoise
    return self.loss_G
```

**æ•ˆæœ**:
- åˆ¤åˆ«å™¨æ˜ç¡®å­¦ä¹ "å™ªéŸ³æ°´å¹³"ç‰¹å¾
- ç”Ÿæˆå™¨è¢«æ˜¾å¼é¼“åŠ±äº§ç”Ÿä½å™ªéŸ³å›¾åƒ
- é¢„æœŸå™ªéŸ³å‡å°‘: **60-80%**

---

### ğŸ¥ˆ æ–¹æ¡ˆB: ä¸¤é˜¶æ®µè®­ç»ƒ

**é˜¶æ®µ1: åŸŸå†…å»å™ª** (å¯é€‰ï¼Œå¦‚æœæœ‰å°‘é‡é…å¯¹æ•°æ®)

å¦‚æœä½ èƒ½è·å¾—ä¸€äº›é…å¯¹çš„å«å™ª/ç›¸å¯¹å¹²å‡€çš„æ•°æ® (ä¾‹å¦‚åŒä¸€æ‚£è€…çš„ä¸åŒæ‰«æ):

```python
# è®­ç»ƒä¸€ä¸ªå»å™ªè‡ªç¼–ç å™¨
class Denoiser(nn.Module):
    def __init__(self):
        super().__init__()
        # U-Netæ¶æ„
        self.encoder = ...
        self.decoder = ...

    def forward(self, noisy_input):
        return self.decoder(self.encoder(noisy_input))

# åœ¨PDåŸŸè®­ç»ƒ
denoiser_A = Denoiser()
for noisy_pd, clean_pd in paired_pd_data:
    loss = ||denoiser_A(noisy_pd) - clean_pd||Â²

# åœ¨PDFsåŸŸè®­ç»ƒ
denoiser_B = Denoiser()
for noisy_pdfs, clean_pdfs in paired_pdfs_data:
    loss = ||denoiser_B(noisy_pdfs) - clean_pdfs||Â²
```

**é˜¶æ®µ2: å¯¹æ¯”åº¦è¿ç§»**

```python
# åœ¨å»å™ªåçš„æ•°æ®ä¸Šè®­ç»ƒUNSB
class MriUnalignedDataset:
    def __init__(self, opt):
        super().__init__(opt)

        # åŠ è½½é¢„è®­ç»ƒçš„å»å™ªå™¨
        self.denoiser_A = load_denoiser('checkpoints/denoiser_A.pth')
        self.denoiser_B = load_denoiser('checkpoints/denoiser_B.pth')
        self.denoiser_A.eval()
        self.denoiser_B.eval()

    def __getitem__(self, index):
        A_tensor = self._load_slice(...)  # å«å™ªPD
        B_tensor = self._load_slice(...)  # å«å™ªPDFs

        # å…ˆå»å™ª
        with torch.no_grad():
            A_clean = self.denoiser_A(A_tensor.unsqueeze(0)).squeeze(0)
            B_clean = self.denoiser_B(B_tensor.unsqueeze(0)).squeeze(0)

        return {'A': A_clean, 'B': B_clean}
```

**ä¼˜ç‚¹**:
- å»å™ªå’Œè¿ç§»åˆ†ç¦»ï¼Œå„è‡ªä¼˜åŒ–
- å¦‚æœå»å™ªå™¨æ•ˆæœå¥½ï¼Œè¿ç§»è´¨é‡ä¼šæ˜¾è‘—æå‡

**ç¼ºç‚¹**:
- éœ€è¦é…å¯¹æ•°æ® (å³ä½¿å°‘é‡)
- ä¸¤é˜¶æ®µè®­ç»ƒå¤æ‚åº¦é«˜
- å»å™ªå¯èƒ½æŸå¤±éƒ¨åˆ†ä¿¡æ¯

---

### ğŸ¥‰ æ–¹æ¡ˆC: è‡ªç›‘ç£å»å™ª (ä¸éœ€è¦é…å¯¹æ•°æ®)

**C1: Noise2Noiseé£æ ¼**

å¦‚æœä½ æœ‰åŒä¸€æ‚£è€…çš„å¤šæ¬¡æ‰«æ:

```python
# è®­ç»ƒæ•°æ®: åŒä¸€è§£å‰–ç»“æ„çš„ä¸¤æ¬¡å«å™ªæ‰«æ
scan1 = noisy_pd_scan1  # Ïƒ â‰ˆ 0.03
scan2 = noisy_pd_scan2  # Ïƒ â‰ˆ 0.03 (ä¸åŒå™ªéŸ³å®ç°)

# è®­ç»ƒç›®æ ‡: ä»scan1é¢„æµ‹scan2
loss = ||denoiser(scan1) - scan2||Â²

# ç¥å¥‡çš„æ˜¯: è¿™ä¼šå­¦åˆ°å»å™ªï¼
# åŸç†: ä¸¤ä¸ªç‹¬ç«‹å™ªéŸ³çš„æœŸæœ›ä¸º0
```

**C2: Noise2Voidé£æ ¼**

å®Œå…¨è‡ªç›‘ç£ï¼Œä¸éœ€è¦å¤šæ¬¡æ‰«æ:

```python
class BlindSpotDenoiser(nn.Module):
    """ç›²ç‚¹ç½‘ç»œ: ä»å‘¨å›´åƒç´ é¢„æµ‹ä¸­å¿ƒåƒç´ """

    def forward(self, x, mask):
        # maskéšæœºé®æŒ¡ä¸€äº›åƒç´ 
        x_masked = x * (1 - mask)

        # ä»å‘¨å›´åƒç´ é¢„æµ‹è¢«é®æŒ¡çš„åƒç´ 
        x_pred = self.network(x_masked)

        return x_pred

# è®­ç»ƒ
for noisy_img in dataset:
    mask = random_mask()  # éšæœºé®æŒ¡10%åƒç´ 

    pred = model(noisy_img, mask)

    # åªåœ¨è¢«é®æŒ¡ä½ç½®è®¡ç®—æŸå¤±
    loss = ||mask * (pred - noisy_img)||Â²
```

**æ•´åˆåˆ°UNSB**:

```python
# sb_model.py
def __init__(self, opt):
    # ... åŸæœ‰ç½‘ç»œ

    # ğŸ”¥ æ–°å¢: è‡ªç›‘ç£å»å™ªæ­£åˆ™åŒ–
    if opt.self_supervised_denoise:
        self.denoiser = BlindSpotDenoiser().to(self.device)
        self.optimizer_denoise = torch.optim.Adam(
            self.denoiser.parameters(), lr=opt.lr
        )

def compute_G_loss(self):
    # ... åŸæœ‰æŸå¤±

    # ğŸ”¥ è‡ªç›‘ç£å»å™ªæŸå¤±
    if self.opt.self_supervised_denoise:
        mask = self.generate_blind_spot_mask()
        denoised = self.denoiser(self.real_A, mask)
        self.loss_denoise = torch.mean(
            mask * (denoised - self.real_A)**2
        )
    else:
        self.loss_denoise = 0.0

    self.loss_G = self.loss_G_GAN + \
                  self.opt.lambda_SB * self.loss_SB + \
                  self.opt.lambda_NCE * self.loss_NCE + \
                  self.opt.lambda_denoise * self.loss_denoise
    return self.loss_G
```

---

## æ¨èå®æ–½è·¯å¾„

åŸºäºä½ çš„"åŒåŸŸéƒ½æœ‰å™ªéŸ³"çš„åœºæ™¯:

### ğŸ¯ é˜¶æ®µ1: åŸºç¡€è‡ªé€‚åº”æ–¹æ³• (1-2å¤©)

å®æ–½**æ–¹æ¡ˆ2** (Nilaå¯å‘çš„è‡ªé€‚åº”SB):
```bash
python train.py \
  --data_noise_level 0.03 \
  --noise_adaptive_schedule linear \
  # ... å…¶ä»–å‚æ•°
```

**é¢„æœŸ**: 30-50%å™ªéŸ³å‡å°‘

---

### ğŸ¯ é˜¶æ®µ2: åˆ¤åˆ«å™¨å¢å¼º (2-3å¤©)

å¦‚æœé˜¶æ®µ1æ•ˆæœä¸å¤Ÿï¼Œæ·»åŠ **æ–¹æ¡ˆA1** (æ•°æ®å¢å¼º):

```bash
python train.py \
  --data_noise_level 0.03 \
  --noise_adaptive_schedule linear \
  --denoise_augmentation \
  --denoise_method lowpass \
  --denoise_prob 0.5 \
  # ... å…¶ä»–å‚æ•°
```

**é¢„æœŸ**: 50-70%å™ªéŸ³å‡å°‘

---

### ğŸ¯ é˜¶æ®µ3: é«˜çº§æ–¹æ³• (5-7å¤©, å¯é€‰)

å¦‚æœä»ä¸æ»¡æ„ï¼Œè€ƒè™‘:

**é€‰é¡¹A**: **æ–¹æ¡ˆA2** (å™ªéŸ³æ¡ä»¶åˆ¤åˆ«å™¨)
- éœ€è¦ä¿®æ”¹ç½‘ç»œæ¶æ„
- æ˜¾å¼å»ºæ¨¡å™ªéŸ³æ°´å¹³
- é¢„æœŸ: 60-80%å™ªéŸ³å‡å°‘

**é€‰é¡¹B**: **æ–¹æ¡ˆC2** (Noise2Void)
- å®Œå…¨è‡ªç›‘ç£
- ä¸éœ€è¦é¢å¤–æ•°æ®
- å¯ä»¥ä¸å…¶ä»–æ–¹æ¡ˆç»„åˆ

---

## è¯„ä¼°æŒ‡æ ‡

### å®šé‡è¯„ä¼°

```python
# å™ªéŸ³æ°´å¹³
from noise_estimation import estimate_noise_mad

input_noise = estimate_noise_mad(input_image)
output_noise = estimate_noise_mad(generated_image)
noise_reduction_ratio = output_noise / input_noise

print(f"Noise reduction: {(1 - noise_reduction_ratio)*100:.1f}%")

# ç›®æ ‡:
# åŸºç¡€æ–¹æ³•: 30-50% reduction
# å¢å¼ºæ–¹æ³•: 50-70% reduction
# é«˜çº§æ–¹æ³•: 60-80% reduction
```

### å®šæ€§è¯„ä¼°

```python
# å¯è§†åŒ–: è¾“å…¥ vs è¾“å‡º vs å™ªéŸ³å›¾
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 4, figsize=(16, 8))

# ç¬¬ä¸€è¡Œ: PDåŸŸ
axes[0, 0].imshow(input_pd_mag, cmap='gray')
axes[0, 0].set_title(f'Input PD\nÏƒ={input_noise_pd:.4f}')

axes[0, 1].imshow(generated_pdfs_mag, cmap='gray')
axes[0, 1].set_title(f'Generated PDFs\nÏƒ={output_noise:.4f}')

axes[0, 2].imshow(reference_pdfs_mag, cmap='gray')
axes[0, 2].set_title(f'Real PDFs\nÏƒ={real_noise_pdfs:.4f}')

# å™ªéŸ³å›¾ (æ®‹å·®çš„é«˜é¢‘æˆåˆ†)
noise_map = input_pd_mag - gaussian_filter(input_pd_mag, sigma=2)
axes[0, 3].imshow(noise_map, cmap='seismic')
axes[0, 3].set_title('Input noise map')

# ç¬¬äºŒè¡Œ: é¢‘è°±åˆ†æ
axes[1, 0].plot(power_spectrum(input_pd_mag), label='Input')
axes[1, 0].plot(power_spectrum(generated_pdfs_mag), label='Generated')
axes[1, 0].set_title('Power Spectrum')
axes[1, 0].set_yscale('log')
axes[1, 0].legend()

plt.tight_layout()
plt.savefig('denoising_evaluation.png')
```

---

## æ€»ç»“

### ä½ çš„åœºæ™¯çš„ç‰¹æ®Šæ€§

```
Nilaåœºæ™¯:     å«å™ªæµ‹é‡ â†’ å¹²å‡€å›¾åƒ (ç›‘ç£ä¿¡å·æ˜ç¡®)
ä½ çš„åœºæ™¯:     å«å™ªPD â†’ å«å™ªPDFs (æ— å¹²å‡€å‚è€ƒ)
```

### èƒ½è¾¾åˆ°çš„æ•ˆæœ

| æ–¹æ¡ˆ | å™ªéŸ³å‡å°‘ | å®æ–½éš¾åº¦ | éœ€è¦é¢å¤–æ•°æ® |
|-----|---------|---------|------------|
| åŸºç¡€è‡ªé€‚åº”SB | 30-50% | ä½ (1-2å¤©) | å¦ |
| + æ•°æ®å¢å¼º | 50-70% | ä¸­ (2-3å¤©) | å¦ |
| + å™ªéŸ³æ¡ä»¶åˆ¤åˆ«å™¨ | 60-80% | é«˜ (5-7å¤©) | å¦ |
| ä¸¤é˜¶æ®µå»å™ª+è¿ç§» | 70-90% | é«˜ (7-10å¤©) | æ˜¯ (é…å¯¹æ•°æ®) |
| + Noise2Void | 60-80% | ä¸­ (3-5å¤©) | å¦ |

### å®è·µå»ºè®®

1. **å…ˆå®æ–½åŸºç¡€æ–¹æ³•** (æ–¹æ¡ˆ2): å¿«é€ŸéªŒè¯æ•ˆæœ
2. **è¯„ä¼°æ˜¯å¦è¶³å¤Ÿ**: å¦‚æœ30-50%å‡å°‘å·²ç»æ»¡è¶³éœ€æ±‚ï¼Œæ— éœ€æ›´å¤æ‚æ–¹æ³•
3. **æ¸è¿›å¢å¼º**: å¦‚æœéœ€è¦æ›´å¥½æ•ˆæœï¼Œé€æ­¥æ·»åŠ å¢å¼ºæŠ€æœ¯
4. **ç›‘æ§trade-off**: è¿‡åº¦å»å™ªå¯èƒ½æŸå¤±å¯¹æ¯”åº¦ç»†èŠ‚

### å…³é”®ç†è§£

**è‡ªé€‚åº”æ–¹æ³•ä¸æ˜¯magic**:
- âœ… èƒ½å‡å°‘å™ªéŸ³ä¼ é€’
- âœ… èƒ½éšå¼å»å™ª
- âŒ ä¸èƒ½å®Œå…¨æ¶ˆé™¤å™ªéŸ³ (é™¤éæœ‰å¹²å‡€å‚è€ƒ)
- âœ… ä½†é…åˆå…¶ä»–æŠ€æœ¯å¯ä»¥è¾¾åˆ°60-80%å™ªéŸ³å‡å°‘

**æœ€é‡è¦çš„**: æ˜ç¡®ä½ çš„ç›®æ ‡
- å¦‚æœä¸»è¦ç›®æ ‡æ˜¯**å¯¹æ¯”åº¦è¿ç§»**, 30-50%å™ªéŸ³å‡å°‘é€šå¸¸è¶³å¤Ÿ
- å¦‚æœä¸»è¦ç›®æ ‡æ˜¯**å»å™ª**, è€ƒè™‘ä¸“é—¨çš„å»å™ªæ–¹æ³•
